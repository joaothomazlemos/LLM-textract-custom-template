"""LLM Service for conversation and chat functionality using Nova Premier.

This module provides the NovaLLMService class for interacting with AWS Bedrock
Nova Premier models through the Converse API, with support for streaming,
caching, and conversation management.
"""

import json
import time
from typing import Any, Dict, Optional
import structlog
from ..utils.request_logger import get_logger

logger = structlog.get_logger(__name__)

class NovaLLMService:
    """Conversation and log inputs needs to use this service class."""
    
    def __init__(
        self,
        model_id: str,
        client: Any,
        system_prompt: str,
        max_tokens: int,
        temperature: float,
        top_p: float = 0.9,
    ) -> None:
        self.model_id = model_id
        self.client = client
        self.system_prompt = system_prompt
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.logger = get_logger().bind(service="api", component="llm-service")


    def _config_conversation(self, input_str: str) -> list:
        """Configure the conversation format for Nova Premier"""
        conversation = [
            {
                "role": "user",
                "content": [{"text": input_str}],
            }
        ]
        return conversation

    def _config_inference_config(self) -> dict:
        """Configure inference parameters for Nova Premier"""
        return {
            "maxTokens": self.max_tokens,
            "temperature": self.temperature,
            "topP": self.top_p,
        }

    def _config_system_prompt(self) -> list:
        """Configure system prompt for Nova Premier with caching enabled"""
        if self.system_prompt:
            system_config = [
                {"text": self.system_prompt},
                {"cachePoint": {"type": "default"}}
            ]
            
            # Log cache configuration for debugging
            self.logger.info(
                "System prompt cache configuration",
                system_prompt_length=len(self.system_prompt),
                system_prompt_size_kb=len(self.system_prompt) // 1024,
                cache_point_configured=True,
                cache_type="default"
            )
            
            return system_config
        return []

    def _format_as_markdown(self, response_text: str, user_query: str, cache_metrics: Optional[Dict[str, int]] = None) -> str:
        """Format the response as markdown with proper structure and cache metrics"""
        markdown_response = f"""
        {response_text}"""

        # Add cache metrics if available
        if cache_metrics:
            cache_read = cache_metrics.get('cache_read_tokens', 0)
            cache_write = cache_metrics.get('cache_write_tokens', 0)
            input_tokens = cache_metrics.get('input_tokens', 0)
            output_tokens = cache_metrics.get('output_tokens', 0)
            
            if cache_read > 0 or cache_write > 0:
                markdown_response += f"""

                ### ðŸ“Š Cache Metrics
                - **Cache Read**: {cache_read} tokens
                - **Cache Write**: {cache_write} tokens  
                - **Input**: {input_tokens} tokens
                - **Output**: {output_tokens} tokens"""

                markdown_response += """

                ---
                *Generated by Nova Premier*
                """
        return markdown_response

    def _validate_prompt_and_log_warnings(self, input_str: str) -> None:
        """Validate prompt length and log warnings for potential issues"""
        # Nova Premier context window limits (approximate)
        MAX_INPUT_TOKENS = 100000  # Rough estimate for Nova Premier
        WARN_INPUT_TOKENS = 80000  # Warning threshold
        
        # System prompt specific limits (should be much smaller)
        MAX_SYSTEM_TOKENS = 20000  # System prompt should be reasonable
        WARN_SYSTEM_TOKENS = 10000  # Warning for large system prompts
        
        # Rough token estimation (1 token â‰ˆ 4 characters for English)
        estimated_input_tokens = len(input_str) // 4
        estimated_system_tokens = len(self.system_prompt) // 4 if self.system_prompt else 0
        total_estimated_tokens = estimated_input_tokens + estimated_system_tokens
        
        # Log prompt statistics
        self.logger.info(
            "Prompt validation",
            input_length=len(input_str),
            system_prompt_length=len(self.system_prompt) if self.system_prompt else 0,
            estimated_input_tokens=estimated_input_tokens,
            estimated_system_tokens=estimated_system_tokens,
            total_estimated_tokens=total_estimated_tokens,
            max_tokens_setting=self.max_tokens
        )
        
        # CRITICAL: Check system prompt size specifically
        if estimated_system_tokens > MAX_SYSTEM_TOKENS:
            self.logger.error(
                "CRITICAL: System prompt is too large - this will cause failures",
                system_prompt_tokens=estimated_system_tokens,
                max_recommended=MAX_SYSTEM_TOKENS,
                system_prompt_size_kb=len(self.system_prompt) // 1024 if self.system_prompt else 0,
                recommendation="Reduce system prompt size significantly or move context to user message"
            )
        elif estimated_system_tokens > WARN_SYSTEM_TOKENS:
            self.logger.warning(
                "Large system prompt detected - may impact performance",
                system_prompt_tokens=estimated_system_tokens,
                recommended_max=WARN_SYSTEM_TOKENS,
                system_prompt_size_kb=len(self.system_prompt) // 1024 if self.system_prompt else 0,
                recommendation="Consider reducing system prompt size"
            )
        
        # Check total context window usage
        if total_estimated_tokens > WARN_INPUT_TOKENS:
            self.logger.warning(
                "Large total prompt detected - may cause slow response or errors",
                total_estimated_tokens=total_estimated_tokens,
                threshold=WARN_INPUT_TOKENS,
                recommendation="Consider splitting the request or reducing context"
            )
            
        if total_estimated_tokens > MAX_INPUT_TOKENS:
            self.logger.error(
                "Prompt may exceed model context window",
                total_estimated_tokens=total_estimated_tokens,
                max_context=MAX_INPUT_TOKENS,
                risk="Request may fail or be truncated"
            )
            
        if self.max_tokens > 4096:
            self.logger.warning(
                "High max_tokens setting may cause slow response",
                max_tokens=self.max_tokens,
                recommendation="Consider reducing max_tokens for faster responses"
            )
            
        # Check for very short prompts that might not be useful
        if len(input_str.strip()) < 10:
            self.logger.warning(
                "Very short prompt detected",
                prompt_length=len(input_str.strip()),
                prompt_content=input_str[:50],
                risk="May produce generic or unhelpful responses"
            )

    def invoke_model(self, input_str: str, streaming: bool = False) -> str:
        """Invoke Nova Premier model using the Converse API with optional streaming"""
        # Validate prompt and log any warnings
        self._validate_prompt_and_log_warnings(input_str)
        
        # Log the start of the request
        self.logger.info(
            "Starting model invocation",
            streaming=streaming,
            input_length=len(input_str),
            model_id=self.model_id
        )
        
        try:
            if streaming:
                result = self._invoke_model_stream(input_str)
            else:
                result = self._invoke_model_standard(input_str)
            
            self.logger.info(
                "Model invocation completed successfully",
                streaming=streaming,
                response_length=len(result) if result else 0
            )
            return result
            
        except Exception as e:
            # Top-level error catch to ensure we never miss an error
            self.logger.error(
                "CRITICAL: Model invocation failed at top level",
                streaming=streaming,
                error_type=type(e).__name__,
                error_message=str(e),
                input_length=len(input_str)
            )
            raise
    
    def _invoke_model_stream(self, input_str: str) -> str:
        """Invoke Nova Premier model using streaming API"""
        try:
            conversation = self._config_conversation(input_str)
            inference_config = self._config_inference_config()
            system = self._config_system_prompt()
            
            # Use the Converse Stream API for Nova Premier
            response = self.client.converse_stream(
                modelId=self.model_id,
                messages=conversation,
                system=system,
                inferenceConfig=inference_config,
            )
            
            # Process streaming response
            raw_response = ""
            cache_metrics = {}
            
            print("ðŸš€ Streaming Response:")
            print("=" * 40)
            
            for chunk in response['stream']:
                # Debug: print chunk structure to understand format
                chunk_keys = list(chunk.keys())
                
                if 'contentBlockDelta' in chunk:
                    # Extract text from content block delta
                    delta = chunk['contentBlockDelta']
                    if 'delta' in delta and 'text' in delta['delta']:
                        text_chunk = delta['delta']['text']
                        raw_response += text_chunk
                        # Print streaming chunks in real-time
                        print(text_chunk, end='', flush=True)
                
                elif 'messageStop' in chunk:
                    # End of message
                    print("\n" + "=" * 40)
                    print("âœ… Streaming Complete")
                    
                elif 'metadata' in chunk:
                    # Extract metadata including usage info
                    metadata = chunk['metadata']
                    usage = metadata.get('usage', {})
                    cache_metrics = {
                        'cache_read_tokens': usage.get("cacheReadInputTokens", 0),
                        'cache_write_tokens': usage.get("cacheWriteInputTokens", 0),
                        'input_tokens': usage.get("inputTokens", 0),
                        'output_tokens': usage.get("outputTokens", 0)
                    }
                    
                # Debug logging for chunk analysis
                if len(raw_response) == 0 and chunk_keys:
                    self.logger.debug(f"Chunk keys: {chunk_keys}")
                    if 'contentBlockDelta' in chunk:
                        self.logger.debug(f"ContentBlockDelta structure: {chunk['contentBlockDelta']}")
            
            # Format as markdown with cache metrics
            markdown_response = self._format_as_markdown(raw_response, input_str, cache_metrics)
            
            # Log successful completion with metrics
            self.logger.info(
                "Nova model invocation completed (streaming)",
                input_length=len(input_str),
                response_length=len(raw_response),
                cache_read_tokens=cache_metrics.get('cache_read_tokens', 0),
                cache_write_tokens=cache_metrics.get('cache_write_tokens', 0),
                input_tokens=cache_metrics.get('input_tokens', 0),
                output_tokens=cache_metrics.get('output_tokens', 0),
                total_tokens=cache_metrics.get('input_tokens', 0) + cache_metrics.get('output_tokens', 0)
            )
            
            return markdown_response
            
        except Exception as e:
            # Enhanced error logging with context
            error_context = {
                "model_id": self.model_id,
                "input_length": len(input_str),
                "system_prompt_length": len(self.system_prompt) if self.system_prompt else 0,
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "top_p": self.top_p,
                "streaming": True,
                "error_type": type(e).__name__,
                "error_message": str(e)
            }
            
            self.logger.error("Nova model invocation failed (streaming)", **error_context)
            
            # Log specific error patterns
            error_str = str(e).lower()
            if "throttl" in error_str or "rate" in error_str:
                self.logger.warning("Rate limiting detected", recommendation="Implement retry with backoff")
            elif "token" in error_str and ("limit" in error_str or "exceed" in error_str):
                self.logger.warning("Token limit exceeded", recommendation="Reduce prompt size or max_tokens")
            elif "timeout" in error_str:
                self.logger.warning("Request timeout", recommendation="Check network or reduce complexity")
                
            raise
    
    def _invoke_model_standard(self, input_str: str) -> str:
        """Invoke Nova Premier model using standard (non-streaming) API"""
        start_time = time.time()
        
        try:
            conversation = self._config_conversation(input_str)
            inference_config = self._config_inference_config()
            system = self._config_system_prompt()
            
            # Debug logging for cache configuration
            self.logger.info(
                "API call configuration",
                model_id=self.model_id,
                system_config_length=len(system),
                system_has_cache_point=any("cachePoint" in item for item in system if isinstance(item, dict)),
                inference_config=inference_config
            )
            
            # Log before API call
            self.logger.info("Calling Bedrock API", timestamp=time.time())
            
            # Use the Converse API for Nova Premier
            response = self.client.converse(
                modelId=self.model_id,
                messages=conversation,
                system=system,
                inferenceConfig=inference_config,
            )
            
            api_call_time = time.time() - start_time
            self.logger.info(
                "Bedrock API call completed",
                api_call_duration_seconds=round(api_call_time, 2)
            )
            
            # Debug the complete response structure
            self.logger.info(
                "Raw API response structure",
                response_keys=list(response.keys()),
                usage_data=response.get("usage", {}),
                has_system_cache=bool(response.get("usage", {}).get("cacheReadInputTokens", 0)),
                response_metadata=response.get("ResponseMetadata", {})
            )
            
            # Extract the response text
            output = response.get("output", {})
            message = output.get("message", {})
            content = message.get("content", [])
            
            # Get the text from the first content block
            raw_response = ""
            if content and len(content) > 0:
                first_content = content[0]
                raw_response = first_content.get("text", "")
            
            # Extract cache metrics from usage
            usage = response.get("usage", {})
            cache_metrics = {
                'cache_read_tokens': usage.get("cacheReadInputTokens", 0),
                'cache_write_tokens': usage.get("cacheWriteInputTokens", 0),
                'input_tokens': usage.get("inputTokens", 0),
                'output_tokens': usage.get("outputTokens", 0)
            }
            
            # Format as markdown with cache metrics
            markdown_response = self._format_as_markdown(raw_response, input_str, cache_metrics)
            
            # Log successful completion with metrics
            total_time = time.time() - start_time
            self.logger.info(
                "Nova model invocation completed (standard)",
                input_length=len(input_str),
                response_length=len(raw_response),
                cache_read_tokens=cache_metrics['cache_read_tokens'],
                cache_write_tokens=cache_metrics['cache_write_tokens'],
                input_tokens=cache_metrics['input_tokens'],
                output_tokens=cache_metrics['output_tokens'],
                total_tokens=cache_metrics['input_tokens'] + cache_metrics['output_tokens'],
                total_duration_seconds=round(total_time, 2),
                api_call_duration_seconds=round(api_call_time, 2)
            )
            
            return markdown_response
            
        except Exception as e:
            # Enhanced error logging with context
            error_context = {
                "model_id": self.model_id,
                "input_length": len(input_str),
                "system_prompt_length": len(self.system_prompt) if self.system_prompt else 0,
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "top_p": self.top_p,
                "streaming": False,
                "error_type": type(e).__name__,
                "error_message": str(e)
            }
            
            self.logger.error("Nova model invocation failed (standard)", **error_context)
            
            # Log specific error patterns
            error_str = str(e).lower()
            if "throttl" in error_str or "rate" in error_str:
                self.logger.warning("Rate limiting detected", recommendation="Implement retry with backoff")
            elif "token" in error_str and ("limit" in error_str or "exceed" in error_str):
                self.logger.warning("Token limit exceeded", recommendation="Reduce prompt size or max_tokens")
            elif "timeout" in error_str:
                self.logger.warning("Request timeout", recommendation="Check network or reduce complexity")
            elif "validation" in error_str or "invalid" in error_str:
                self.logger.warning("Validation error", recommendation="Check prompt format and parameters")
                
            raise
